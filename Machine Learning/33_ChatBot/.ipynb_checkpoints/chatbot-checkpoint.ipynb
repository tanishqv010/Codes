{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02610c0d-66ec-4d7d-9ae5-536c52deb722",
   "metadata": {},
   "source": [
    "<h3>Importing the Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252a9bc0-648e-4671-aeaa-b64c25641f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a39750-f039-4537-a665-00074066cbba",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734b6ab2-f79a-496d-a150-bcd220531f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"movie_lines.txt\", encoding = \"utf-8\", errors = \"ignore\").read().split(\"\\n\")\n",
    "conversations = open(\"movie_conversations.txt\", encoding = \"utf-8\", errors = \"ignore\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f76f9c-ffd2-4184-aeb2-e21467e8b98a",
   "metadata": {},
   "source": [
    "<h4>Creating a dictionary that maps each line and its id</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa45f3a3-e421-4640-a907-e35702353192",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(\" +++$+++ \")\n",
    "    if(len(_line) == 5):\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61da929-ec40-495c-ae5c-faa897ed8c02",
   "metadata": {},
   "source": [
    "<h4>Creating a list of all of the conversations</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0baa1716-45be-447f-9739-b460fe7faf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90eb490-108d-4fdf-9e23-39375a84f8f8",
   "metadata": {},
   "source": [
    "<h4>Getting Separately the Questions and Answers</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593d8e16-3954-4d4e-bb86-c92f4b117ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c470e76-baa2-4e67-be8a-b80f6876025b",
   "metadata": {},
   "source": [
    "<h4>First Cleaning of the Texts</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3afa84dd-8966-4d0e-b8b9-a227fc94338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \"are\", text)\n",
    "    text = re.sub(r\"\\'d\", \"would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"[~()\\\"#/@;:<>{}+=-|.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dff0b0-ab86-4e16-b75b-919e0c581645",
   "metadata": {},
   "source": [
    "<h4>Cleaning the Questions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a58dd3-d30a-4d71-b1f8-a32bfb0b3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b143c9-0e9b-4c35-abb1-9bd78891cfbf",
   "metadata": {},
   "source": [
    "<h4>Cleaning the Answers</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c547ab03-4bfd-4336-a441-8db4ab610453",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ce34a-b3e9-4262-8d5c-7c659d3da33c",
   "metadata": {},
   "source": [
    "<h4>Creating a dictionary that maps each word to its number of occurances</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4a4a88-3f67-403b-ae8e-a1c23a3bde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for question in clean_questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0aa74e-384b-4028-ad3d-34a6a3f71074",
   "metadata": {},
   "source": [
    "<h4>Creating two Dictionaries that map the Questions Words and the Answers Words to a unique integer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa1d6c6-7587-4721-8551-0b6cd1e34a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "answerswords2int = {}\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023f864-de93-40fb-935c-6cc98fa6ff17",
   "metadata": {},
   "source": [
    "<h4>Adding the Last Tokens to these two Dictionaries</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3a0e24-80ec-4b56-a14e-aa6c86de370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ecb4c-3b18-49b0-a92c-b803f7d144c4",
   "metadata": {},
   "source": [
    "<h4>Creating the inverse dictionary of the answerswords2int dictionary</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c1810ce-3e16-4b40-970d-508e42bffe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e9986-e222-4457-be5f-24848fd732a8",
   "metadata": {},
   "source": [
    "<h4>Adding the End Of String token to the end of every answer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2016000d-b37a-4a6a-bfb1-43a29f6ac5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(clean_answers)):\n",
    "    clean_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713c2a3-e35f-4642-b941-772effe53fd5",
   "metadata": {},
   "source": [
    "<h4>Translating all the questions and the answers into integers  and Replacing all the words that were filtered out </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ad058-506b-4a7f-b5c7-bea38f7bef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_to_int = []\n",
    "for question in clean_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_to_int.append(ints)\n",
    "answers_to_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_to_int.append(ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa3a8c-edec-44b5-abbb-210c60705e7c",
   "metadata": {},
   "source": [
    "<h4>Sorting Questions and Answers by length of Questions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb8d7a-c535-4e0c-85b9-2b0501cfd833",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "for length in range(1, 25 + 1):\n",
    "    for i in enumerate(questions_to_int):\n",
    "        if(len(i[1]) == length):\n",
    "            sorted_clean_questions.append(questions_to_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_to_int[i[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5c33f-c68e-4595-aa32-ba3ab66f5b11",
   "metadata": {},
   "source": [
    "<h3>Building the Seq2seq Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6301921-9157-4f3b-875a-7b663b822d6f",
   "metadata": {},
   "source": [
    "<h4>Creating placehoders for the input and the targets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0525c2-7577-42fa-9cbb-92a83f4ae7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs = tf.compat.v1.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.compat.v1.placeholder(tf.int32, [None, None], name='target')\n",
    "    lr = tf.compat.v1.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa05c53-1456-44a5-a40f-606c8546da78",
   "metadata": {},
   "source": [
    "<h4>Preprocessing the targets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4c97e-13b0-4ba7-8879-51a7dc6c76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772bc6f-9b50-4c63-b4f3-5fa165d31e96",
   "metadata": {},
   "source": [
    "<h4>Creating the Encoder RNN Layer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54485e-e9f0-4393-b149-35f607af3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_rnn_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encode_cell,\n",
    "                                                      cell_bw = encoder_cell,\n",
    "                                                      sequence_length = sequence_length,\n",
    "                                                      inputs = rnn_inputs,\n",
    "                                                      dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60c827-2b3c-46a3-9edc-666c600a1908",
   "metadata": {},
   "source": [
    "<h4>Decoding the Training Set</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9dd741-b156-4918-b1bf-ef9ee230335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeroes([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = 'bahdanau', num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                             name = 'attn_dec_train')\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                             training_decoder_function,\n",
    "                                                                                                             decoder_embedded_inputs,\n",
    "                                                                                                             sequence_length,\n",
    "                                                                                                             scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47513a5d-9c09-4904-a2c7-b2fd62d1f134",
   "metadata": {},
   "source": [
    "<h4>Decoding the Test / Validation Set</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78336622-5a67-4ef2-bf5c-99fb9f818ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeroes([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = 'bahdanau', num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                                  encoder_state[0],\n",
    "                                                                                  attention_keys,\n",
    "                                                                                  attention_values,\n",
    "                                                                                  attention_score_function,\n",
    "                                                                                  attention_construct_function,\n",
    "                                                                                  decoder_embeddings_matrix,\n",
    "                                                                                  sos_id,\n",
    "                                                                                  eos_id,\n",
    "                                                                                  maximum_length,\n",
    "                                                                                  num_words,\n",
    "                                                                                  name = 'attn_dec_inf')\n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                                                                test_decoder_function,\n",
    "                                                                                                                scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf868a-795b-48c1-baec-43487df127dc",
   "metadata": {},
   "source": [
    "<h4>Creating the Decoder RNN</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65154a87-31df-41d3-9dfa-8b5fca920dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeroes_initializer()\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                     num_words,\n",
    "                                                                     activation_function = 'relu',\n",
    "                                                                     normallization = None,\n",
    "                                                                     scope = decoding_scope,\n",
    "                                                                     weights_initializers = weights,\n",
    "                                                                     biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                  decoder_cell,\n",
    "                                                  decoder_embedded_input,\n",
    "                                                  sequence_length,\n",
    "                                                  decoding_scope,\n",
    "                                                  output_function, \n",
    "                                                  keep_prob,\n",
    "                                                  batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                          decoder_cell,\n",
    "                                          decoder_embedded_matrix,\n",
    "                                          word2int['<SOS>'],\n",
    "                                          word2int['<EOS>'],\n",
    "                                          sequence_length - 1,\n",
    "                                          num_words,\n",
    "                                          decoding_scope,\n",
    "                                          output_function,\n",
    "                                          keep_prob,\n",
    "                                          batch_size)\n",
    "        return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4caee-01a8-4b14-a1f4-7c70fefef7c0",
   "metadata": {},
   "source": [
    "<h4>Building the Seq2Seq Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef9a52-bb0f-4819-ad0c-039b5dc3b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765c89a-d127-4533-92f6-de94c699034b",
   "metadata": {},
   "source": [
    "<h3>Training the Seq2Seq Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e415d1-84b5-4ba9-afab-7058d39fc2b1",
   "metadata": {},
   "source": [
    "<h4>Setting the Hyperparameters</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ed2ba-8034-4bd2-b0f9-44484066f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9a1be-110e-480c-b287-e9e9890a7bd9",
   "metadata": {},
   "source": [
    "<h4>Defining a Session</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed277587-fbb2-42b5-bc8c-889f37425ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "session = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b18652-326f-446c-9a17-b9f90fd6c672",
   "metadata": {},
   "source": [
    "<h4>Loading the Model Inputs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3556b-44bb-4bdb-9fb8-3f5021e5776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, lr, keep_prob = model_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ce3b5-9fba-4af3-8b59-fd9f8a8d94ce",
   "metadata": {},
   "source": [
    "<h4>Setting the Sequence Length</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692324b9-bb9a-415f-9783-f9ecfb2bb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = tf.compat.v1.placeholder_with_default(25, None, name = 'sequence_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f35408-4bea-4ec0-bd79-d9d666844b55",
   "metadata": {},
   "source": [
    "<h4>Getting the shapes of the input tensor</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c6aa9-9427-437f-8ea9-95a4d61ac1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec1b15-ff24-4849-95ca-a9dde950fba6",
   "metadata": {},
   "source": [
    "<h4>Getting the Training and the Test Predictions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3ab08-bc86-4520-85eb-9112244793db",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                      targets,\n",
    "                                                      keep_prob,\n",
    "                                                      batch_size,\n",
    "                                                      sequence_length,\n",
    "                                                      len(answerswords2int),\n",
    "                                                      len(questionswords2int),\n",
    "                                                      encoding_embedding_size,\n",
    "                                                      decoding_embedding_size,\n",
    "                                                      rnn_size,\n",
    "                                                      num_layers,\n",
    "                                                      questionswords2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97453af3-64ee-4b3d-8dce-c6cf8aea55de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
